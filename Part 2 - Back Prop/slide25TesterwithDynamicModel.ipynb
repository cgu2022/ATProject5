{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit35d7a11e2e4c46fa9a25217bb1d0a4e1",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input Data\n",
    "X = np.array(([[0.05, 0.05, 0.05], [0.1, 0.1, 0.1]]), dtype=float) # 2, 3\n",
    "y = np.array([[0.01, 0.01, 0.01], [0.99, 0.99, 0.99]], dtype=float) # 2, 3\n",
    "\n"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 11
  },
  {
   "source": [
    "class Model3:\n",
    "\n",
    "    def __init__(self, model_structure=[2, 3, 2], modelActivationFunctions=[\"sigmoid\", \"sigmoid\"]):\n",
    "\n",
    "        self.model_structure = model_structure\n",
    "        self.modelActivationFunctions = modelActivationFunctions\n",
    "        self.modelWidth = len(model_structure)\n",
    "\n",
    "        # Safety Check to make sure model structure is legitimate\n",
    "        if self.modelWidth<3 or self.modelWidth-1!=len(self.modelActivationFunctions):\n",
    "            print(\"Model Structure Error!\")\n",
    "            exit(1)  \n",
    "\n",
    "        # Weights (Parameters) - Randomly Assigned\n",
    "        self.weights = list()\n",
    "        self.weights.append(np.random.randn(self.model_structure[1],self.model_structure[0])) # Input Layer Weights\n",
    "        for i in range(1, self.modelWidth-1):\n",
    "            self.weights.append(np.random.randn(self.model_structure[i+1], self.model_structure[i]))\n",
    "\n",
    "        # Biases - Randomly Assigned\n",
    "        self.biases = list()\n",
    "        for i in range(1, self.modelWidth):\n",
    "            self.biases.append(np.random.randn(self.model_structure[i], 1))  # Length should be number of columns of X\n",
    "        '''self.weights = [np.array(([[0.15, 0.3], [0.2, 0.35], [0.25, 0.4]]), dtype=float), np.array(([[0.5, 0.6, 0.7], [0.55, 0.65, 0.75]]), dtype=float)]\n",
    "        self.biases = [[[0.45], [0.45], [0.45]], [[0.8], [0.8]]]'''\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "            return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self, z):\n",
    "        # Derivative of Sigmoid Function\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z)\n",
    "        sm = (np.exp(z).T / np.sum(np.exp(z), axis=0)).T\n",
    "        return sm\n",
    "        \n",
    "    '''\n",
    "    # https://aerinykim.medium.com/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d\n",
    "    def softmax(z):\n",
    "        z -= np.max(z)\n",
    "        sm = (np.exp(z).T / np.sum(np.exp(z), axis=0)).T\n",
    "        return sm\n",
    "    '''\n",
    "    def softmaxPrime(self, z):\n",
    "        soft_max = self.softmax(x)\n",
    "        # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "        s = soft_max.reshape(-1,1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def reluPrime(self, x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "    \n",
    "    def applyActivationFunction(self, values, activation):\n",
    "        if activation=='sigmoid':\n",
    "            return self.sigmoid(values)\n",
    "        elif activation=='softmax':\n",
    "            return self.softmax(values)\n",
    "        elif activation=='relu':\n",
    "            return self.relu(values)\n",
    "        else:\n",
    "            print(\"Unknown Activation Function! Got:\", activation)\n",
    "            exit(1)\n",
    "    \n",
    "    def applyActivationFunctionPrime(self, values, activation):\n",
    "        if activation=='sigmoid':\n",
    "            return self.sigmoidPrime(values)\n",
    "        elif activation=='softmax':\n",
    "            return self.softmaxPrime(values)\n",
    "        elif activation=='relu':\n",
    "            return self.reluPrime(values)\n",
    "        else:\n",
    "            print(\"Unknown Activation Function! Got:\", activation)\n",
    "            exit(1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propogate inputs through networks\n",
    "        self.aValues = list()\n",
    "        self.zValues = list()\n",
    "        self.aValues.append(X) # First a value = input values\n",
    "        for i in range(0, len(self.weights)):\n",
    "            self.zValues.append(np.dot(self.weights[i], self.aValues[i]) + self.biases[i])\n",
    "            self.aValues.append(self.applyActivationFunction(self.zValues[i], self.modelActivationFunctions[i]))\n",
    "        yHat = self.aValues[-1]\n",
    "        return yHat\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        # Compute cost using the weights already stored\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "    \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # Computes partial derivatives of Cost function with respect to weights & biases\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        weightDerivatives = list() # Derivative of Cost function with respect to weights\n",
    "        biasDerivatives = list() # Derivative of Cost function with respect to biases\n",
    "\n",
    "        # Last Layer derivatives\n",
    "        delta = np.multiply(-(y-self.aValues[-1]), self.sigmoidPrime(self.zValues[-1]))\n",
    "        weightDerivatives.insert(0, np.dot(delta, self.aValues[-2].T))\n",
    "        biasDerivatives.insert(0, delta.sum(axis=1).reshape(delta.shape[0],1)) \n",
    "        \n",
    "        # Derivatives for the other layers (L-1, L-2, ...)\n",
    "        for i in range(self.modelWidth-2, 0, -1):\n",
    "            delta = np.multiply(np.dot(self.weights[i].T, delta), self.applyActivationFunctionPrime(self.zValues[i-1], self.modelActivationFunctions[i])) \n",
    "            weightDerivatives.insert(0, np.dot(delta, self.aValues[i-1].T))\n",
    "            biasDerivatives.insert(0, delta.sum(axis=1).reshape(delta.shape[0],1))\n",
    "\n",
    "        return weightDerivatives, biasDerivatives\n",
    "    \n",
    "    def tuneParams(self, X, y, learning_rate=0.5):\n",
    "        # Get Derivatives of Weights & Biases, and then adjust weights/biases with learning rate*derivatives\n",
    "        self.weightDerivatives, self.biasDerivatives = self.costFunctionPrime(X, y)\n",
    "        scalar = learning_rate/X.shape[1] # learning rate divided by number of samples\n",
    "        for i in range(0, len(self.weights)):\n",
    "            self.weights[i] = self.weights[i] - scalar*self.weightDerivatives[i]\n",
    "            self.biases[i] = self.biases[i] - scalar*self.biasDerivatives[i]\n"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 12
  },
  {
   "source": [
    "#model = Model3()\n",
    "model = Model3(model_structure=[2, 5, 3, 2], modelActivationFunctions=['relu', 'relu', 'sigmoid']) # Another test"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 17
  },
  {
   "source": [
    "cost1 = model.costFunction(X, y)\n",
    "print(\"Before:\", cost1)\n",
    "model.tuneParams(X, y)\n",
    "cost4 = model.costFunction(X, y)\n",
    "print(\"After:\", cost4)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Before: [0.35089486 0.35089486 0.35089486]\nAfter: [0.29235721 0.29235721 0.29235721]\n"
    }
   ],
   "metadata": {},
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Random Stuff (mainly for debugging)\n"
   ]
  },
  {
   "source": [
    "print(np.random.randn(3, 1))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1.53347245]\n [1.14802192]\n [1.05946144]]\n"
    }
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "source": [
    "cost1 = model.costFunction(X, y)\n",
    "print(cost1)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.17626944 0.17626944 0.17626944]\n"
    }
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "source": [
    "thing = model.aValues\n",
    "for i in range(len(thing)):\n",
    "    print(str(i)+\": \")\n",
    "    print(thing[i])\n",
    "    print()\n"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0: \n[[0.05 0.05 0.05]\n [0.1  0.1  0.1 ]]\n\n1: \n[[0.45495606 0.45495606 0.45495606]\n [0.13651343 0.13651343 0.13651343]\n [0.44382312 0.44382312 0.44382312]\n [0.16402444 0.16402444 0.16402444]\n [0.21399519 0.21399519 0.21399519]]\n\n2: \n[[0.15912218 0.15912218 0.15912218]\n [0.         0.         0.        ]\n [0.         0.         0.        ]]\n\n3: \n[[0.5759319  0.5759319  0.5759319 ]\n [0.47727111 0.47727111 0.47727111]]\n\n"
    }
   ],
   "metadata": {},
   "execution_count": 423
  },
  {
   "source": [
    "model.biases"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array([[-0.03573841],\n        [-0.17444797],\n        [-1.28903062]]), array([[ 1.06369257],\n        [-0.1038854 ]])]"
     },
     "metadata": {},
     "execution_count": 388
    }
   ],
   "metadata": {},
   "execution_count": 388
  },
  {
   "source": [
    "model.weights[1]"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.85795118, -0.53548953, -0.42762427,  0.53554238,  0.2337851 ],\n       [-0.17523553,  0.2969102 ,  0.79902637, -1.26389863, -1.04866783],\n       [ 0.2601352 ,  1.48739739, -0.8230129 ,  0.49066952,  2.07740798]])"
     },
     "metadata": {},
     "execution_count": 483
    }
   ],
   "metadata": {},
   "execution_count": 483
  },
  {
   "source": [
    "model.weightDerivatives[0]"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.00101457, -0.00202913],\n       [ 0.        ,  0.        ],\n       [-0.0005538 , -0.00110759],\n       [ 0.00097552,  0.00195104],\n       [ 0.0013559 ,  0.00271181]])"
     },
     "metadata": {},
     "execution_count": 487
    }
   ],
   "metadata": {},
   "execution_count": 487
  },
  {
   "source": [
    "model.weights[0] - model.weightDerivatives[0]"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.71622685,  0.08222378],\n       [ 2.30359731, -0.66182807],\n       [ 0.31348833,  0.49001116],\n       [-0.55052604, -0.41989909],\n       [ 0.04692531,  0.06503582]])"
     },
     "metadata": {},
     "execution_count": 485
    }
   ],
   "metadata": {},
   "execution_count": 485
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}